{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Symbolic regression via neural network weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create equations and respective datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create random quadratic equations in the form +/- ax^2 +/- bx +/- c; and generate X, y pairs for the said equations using the following routine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random_quadratics import RandomQuadratic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_bound = -5\n",
    "upper_bound = 5\n",
    "round_digits = 1\n",
    "number_of_equations = 6000\n",
    "\n",
    "equations = []\n",
    "\n",
    "for _ in range(number_of_equations):\n",
    "    equation = RandomQuadratic(lower_bound=lower_bound, upper_bound=upper_bound, round_digits=round_digits)\n",
    "    equations.append(equation())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import uniform\n",
    "from generate_datasets import GenerateDatasets\n",
    "from json import dumps\n",
    "from os import mkdir, path\n",
    "\n",
    "X_values = uniform(-1, 1, 5000).astype(dtype=\"float64\", copy=False)\n",
    "root_dir = \"./datasets\"\n",
    "dataset_generator = GenerateDatasets(equations=equations, X_values=X_values)\n",
    "datesets_dict = dataset_generator.generate_xy_datasets()\n",
    "# dumps makes the float X values in keys into strings\n",
    "\n",
    "print(dumps(datesets_dict, indent=4))\n",
    "GenerateDatasets.write_dataset_to_file(datesets_dict, root_dir=root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the saved X,y pairs for the respective equations to pytorch dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from equations_dataset import EquationsDataset\n",
    "from json import load\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch import set_printoptions\n",
    "\n",
    "root_dir = \"./datasets\"\n",
    "equation_data = EquationsDataset(dataset_file_path=f\"{root_dir}/Equation5.json\")\n",
    "#data_loader = DataLoader(equation_data, batch_size=1, shuffle=True)\n",
    "#for idx, xy_values in enumerate(data_loader):\n",
    "    #print(f\"XY at position {idx} is {xy_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(data_loader)\n",
    "#print(equation_data.x_values)\n",
    "\n",
    "#train, test = random_split(data_loader, [4000, 1000])\n",
    "\n",
    "#print(vars(train))\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "#train_loader = DataLoader(train, batch_size=1, shuffle=True)\n",
    "#test_loader = DataLoader(test, batch_size=1, shuffle=True)\n",
    "#print(vars(train_loader))\n",
    "#print(train_loader.X)\n",
    "\n",
    "train_values_x, train_values_y = equation_data.x_values[:4000], equation_data.y_values[:4000] \n",
    "test_values_x, test_values_y = equation_data.x_values[4000:], equation_data.x_values[4000:] \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_values_x, train_values_y, test_size=0.33, random_state=42)\n",
    "\n",
    "train_data = []\n",
    "for i in range(len(X_train)):\n",
    "    train_data.append([X_train[i], y_train[i]])\n",
    "\n",
    "\n",
    "\n",
    "val_data = []\n",
    "for i in range(len(X_val)):\n",
    "    val_data.append([X_val[i], y_val[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=1, shuffle=True)\n",
    "#for idx, xy_values in enumerate(train_loader):\n",
    "    #print(f\"XY at position {idx} is {xy_values}\")\n",
    "\n",
    "val_loader = DataLoader(val_data, batch_size=1, shuffle=True)\n",
    "#for idx, xy_values in enumerate(val_loader):\n",
    "    #print(f\"XY at position {idx} is {xy_values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple MLP to regression on our tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_dataloader:DataLoader, validation_dataloader:DataLoader, epochs, model:nn.Module, optimizer, scheduler, criterion):\n",
    "    train_losses = []\n",
    "    validation_losses = []\n",
    "\n",
    "    #train-validation loop\n",
    "    for epoch in range(epochs):\n",
    "        batch_losses = []\n",
    "        training_loss = 0.0\n",
    "        #training loop\n",
    "        for _idx , data in enumerate(train_dataloader):\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            model.train()\n",
    "            outputs = model(inputs.float())\n",
    "            loss = criterion(outputs.float(), labels.float())\n",
    "            loss.backward()\n",
    "            batch_losses.append(loss.item())\n",
    "            optimizer.step()\n",
    "        training_loss = np.mean(batch_losses)\n",
    "        train_losses.append(training_loss)\n",
    "        scheduler.step()\n",
    "\n",
    "        #validation loop\n",
    "        with torch.no_grad():\n",
    "            val_losses = []\n",
    "            validation_loss = 0.0\n",
    "            for _idx, data in enumerate(validation_dataloader):\n",
    "                inputs, labels = data\n",
    "                model.eval()\n",
    "                outputs = model(inputs.float())\n",
    "                loss = criterion(outputs.float(), labels.float())\n",
    "                val_losses.append(loss.item())\n",
    "            validation_loss = np.mean(val_losses)\n",
    "            validation_losses.append(validation_loss)\n",
    "\n",
    "        print(f\"[{epoch+1}] Training loss: {training_loss:.7f}\\t Validation loss: {validation_loss:.7f}\")\n",
    "        print(f\"\\t Label value: {labels.float().item()}\\t Predicted Output: {outputs.float().item()}\")\n",
    "    #torch.save(model.state_dict(), MODEL_PATH)\n",
    "    return model.state_dict()\n",
    "\n",
    "def eval_model(test_dataloader: DataLoader, model: nn.Module, criterion):\n",
    "    test_losses = []\n",
    "    with torch.no_grad():\n",
    "        for _idx, data in enumerate(test_dataloader):\n",
    "            inputs, labels = data\n",
    "            model.eval()\n",
    "            outputs = model(inputs)\n",
    "            #print(\"outputs, \", outputs.shape)\n",
    "            #rescaled_outputs = inverse_scaler(outputs, method=\"minmax\")\n",
    "            #print(\"rescaled_outputs: \",rescaled_outputs.shape)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_losses.append(loss.item())\n",
    "        test_loss = np.mean(test_losses)\n",
    "        print(f\"Final test loss: {test_loss:.4f}\")    \n",
    "    return test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test_MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # call constructor from superclass\n",
    "        super(Test_MLP, self).__init__()\n",
    "        # define network layers\n",
    "        self.input_size = input_size\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, 7)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(7, hidden_size)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc4 = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # define forward pass\n",
    "        output = self.fc1(x)\n",
    "        output = self.relu1(output)\n",
    "        output = self.fc2(output)\n",
    "        output = self.relu2(output)\n",
    "        output = self.fc3(output)\n",
    "        output = self.relu3(output)\n",
    "        output = self.fc4(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test_MLP(\n",
      "  (fc1): Linear(in_features=1, out_features=8, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (fc2): Linear(in_features=8, out_features=7, bias=True)\n",
      "  (relu2): ReLU()\n",
      "  (fc3): Linear(in_features=7, out_features=8, bias=True)\n",
      "  (relu3): ReLU()\n",
      "  (fc4): Linear(in_features=8, out_features=1, bias=True)\n",
      ")\n",
      "[1] Training loss: 6.2270619\t Validation loss: 5.8835313\n",
      "\t Label value: 3.1099154949188232\t Predicted Output: 0.579146683216095\n",
      "[2] Training loss: 5.6806706\t Validation loss: 5.3024153\n",
      "\t Label value: 3.674118995666504\t Predicted Output: 0.6968907117843628\n",
      "[3] Training loss: 5.0531033\t Validation loss: 4.6547504\n",
      "\t Label value: 2.8657939434051514\t Predicted Output: 0.8297504782676697\n",
      "[4] Training loss: 4.3681413\t Validation loss: 3.9620186\n",
      "\t Label value: 3.74953293800354\t Predicted Output: 1.120961308479309\n",
      "[5] Training loss: 3.6687089\t Validation loss: 3.2836861\n",
      "\t Label value: 3.5961296558380127\t Predicted Output: 1.2942225933074951\n",
      "[6] Training loss: 2.9847603\t Validation loss: 2.6178079\n",
      "\t Label value: 3.3987863063812256\t Predicted Output: 1.486496090888977\n",
      "[7] Training loss: 2.3286043\t Validation loss: 2.0011770\n",
      "\t Label value: 2.7942612171173096\t Predicted Output: 1.6089527606964111\n",
      "[8] Training loss: 1.7442000\t Validation loss: 1.4765722\n",
      "\t Label value: 3.5874383449554443\t Predicted Output: 2.366490125656128\n",
      "[9] Training loss: 1.2679729\t Validation loss: 1.0747343\n",
      "\t Label value: 0.2174544632434845\t Predicted Output: 1.6011203527450562\n",
      "[10] Training loss: 0.9259456\t Validation loss: 0.8072628\n",
      "\t Label value: 3.6929564476013184\t Predicted Output: 2.7358756065368652\n",
      "[11] Training loss: 0.7132807\t Validation loss: 0.6514071\n",
      "\t Label value: 3.4095826148986816\t Predicted Output: 3.472368001937866\n",
      "[12] Training loss: 0.5917939\t Validation loss: 0.5613561\n",
      "\t Label value: 0.0809207633137703\t Predicted Output: 1.6851904392242432\n",
      "[13] Training loss: 0.5170370\t Validation loss: 0.4991012\n",
      "\t Label value: 3.457643508911133\t Predicted Output: 3.821439743041992\n",
      "[14] Training loss: 0.4610339\t Validation loss: 0.4476779\n",
      "\t Label value: 3.4390079975128174\t Predicted Output: 3.9311044216156006\n",
      "[15] Training loss: 0.4135521\t Validation loss: 0.4040206\n",
      "\t Label value: 3.5913729667663574\t Predicted Output: 3.866896629333496\n",
      "[16] Training loss: 0.3741320\t Validation loss: 0.3686957\n",
      "\t Label value: 1.7144227027893066\t Predicted Output: 1.8970288038253784\n",
      "[17] Training loss: 0.3418151\t Validation loss: 0.3389581\n",
      "\t Label value: 3.7167599201202393\t Predicted Output: 3.769904613494873\n",
      "[18] Training loss: 0.3144881\t Validation loss: 0.3146781\n",
      "\t Label value: 1.776850700378418\t Predicted Output: 1.8331302404403687\n",
      "[19] Training loss: 0.2926732\t Validation loss: 0.2951591\n",
      "\t Label value: 1.0404638051986694\t Predicted Output: 1.481598138809204\n",
      "[20] Training loss: 0.2748316\t Validation loss: 0.2795696\n",
      "\t Label value: -0.20749615132808685\t Predicted Output: 1.0522629022598267\n",
      "[21] Training loss: 0.2612204\t Validation loss: 0.2677201\n",
      "\t Label value: 2.886937379837036\t Predicted Output: 2.4342153072357178\n",
      "[22] Training loss: 0.2500961\t Validation loss: 0.2575393\n",
      "\t Label value: 3.742654323577881\t Predicted Output: 3.7635083198547363\n",
      "[23] Training loss: 0.2404376\t Validation loss: 0.2482320\n",
      "\t Label value: -0.156337708234787\t Predicted Output: 0.9759823083877563\n",
      "[24] Training loss: 0.2315341\t Validation loss: 0.2394806\n",
      "\t Label value: 3.591938018798828\t Predicted Output: 4.018980026245117\n",
      "[25] Training loss: 0.2231707\t Validation loss: 0.2310049\n",
      "\t Label value: 1.3396345376968384\t Predicted Output: 1.4807417392730713\n",
      "[26] Training loss: 0.2151841\t Validation loss: 0.2229242\n",
      "\t Label value: 0.8836586475372314\t Predicted Output: 1.251010537147522\n",
      "[27] Training loss: 0.2075967\t Validation loss: 0.2154163\n",
      "\t Label value: 3.6792941093444824\t Predicted Output: 3.428018808364868\n",
      "[28] Training loss: 0.2004145\t Validation loss: 0.2081130\n",
      "\t Label value: 3.4658923149108887\t Predicted Output: 4.111724376678467\n",
      "[29] Training loss: 0.1933849\t Validation loss: 0.2010597\n",
      "\t Label value: -0.3897005617618561\t Predicted Output: 0.8844789266586304\n",
      "[30] Training loss: 0.1867117\t Validation loss: 0.1939650\n",
      "\t Label value: -0.36062586307525635\t Predicted Output: 0.8702022433280945\n",
      "[31] Training loss: 0.1801221\t Validation loss: 0.1870825\n",
      "\t Label value: 0.7717157602310181\t Predicted Output: 1.1293357610702515\n",
      "[32] Training loss: 0.1737186\t Validation loss: 0.1804443\n",
      "\t Label value: 1.9307386875152588\t Predicted Output: 1.781546711921692\n",
      "[33] Training loss: 0.1674947\t Validation loss: 0.1740168\n",
      "\t Label value: 3.4173877239227295\t Predicted Output: 4.124516487121582\n",
      "[34] Training loss: 0.1613666\t Validation loss: 0.1676864\n",
      "\t Label value: 0.15943297743797302\t Predicted Output: 0.8094609975814819\n",
      "[35] Training loss: 0.1555127\t Validation loss: 0.1615570\n",
      "\t Label value: 2.8029892444610596\t Predicted Output: 2.5298781394958496\n",
      "[36] Training loss: 0.1497104\t Validation loss: 0.1556206\n",
      "\t Label value: 3.0167696475982666\t Predicted Output: 2.7643845081329346\n",
      "[37] Training loss: 0.1440953\t Validation loss: 0.1498216\n",
      "\t Label value: 3.7414469718933105\t Predicted Output: 3.563220500946045\n",
      "[38] Training loss: 0.1385345\t Validation loss: 0.1444091\n",
      "\t Label value: 3.754305601119995\t Predicted Output: 3.6671252250671387\n",
      "[39] Training loss: 0.1333424\t Validation loss: 0.1387101\n",
      "\t Label value: 3.7330522537231445\t Predicted Output: 3.765096664428711\n",
      "[40] Training loss: 0.1281868\t Validation loss: 0.1334202\n",
      "\t Label value: 3.6840803623199463\t Predicted Output: 3.856842279434204\n",
      "[41] Training loss: 0.1232550\t Validation loss: 0.1283125\n",
      "\t Label value: 2.106038808822632\t Predicted Output: 1.9558721780776978\n",
      "[42] Training loss: 0.1184129\t Validation loss: 0.1233635\n",
      "\t Label value: 3.5185365676879883\t Predicted Output: 3.2891104221343994\n",
      "[43] Training loss: 0.1138507\t Validation loss: 0.1187427\n",
      "\t Label value: -0.23427380621433258\t Predicted Output: 0.6575225591659546\n",
      "[44] Training loss: 0.1094066\t Validation loss: 0.1140633\n",
      "\t Label value: 3.4282267093658447\t Predicted Output: 3.214444160461426\n",
      "[45] Training loss: 0.1051984\t Validation loss: 0.1096932\n",
      "\t Label value: 3.5648324489593506\t Predicted Output: 3.962742567062378\n",
      "[46] Training loss: 0.1011271\t Validation loss: 0.1054963\n",
      "\t Label value: 3.5186922550201416\t Predicted Output: 3.293250322341919\n",
      "[47] Training loss: 0.0972219\t Validation loss: 0.1014965\n",
      "\t Label value: 3.6033294200897217\t Predicted Output: 3.910691022872925\n",
      "[48] Training loss: 0.0934650\t Validation loss: 0.0976201\n",
      "\t Label value: 3.583264112472534\t Predicted Output: 3.3561127185821533\n",
      "[49] Training loss: 0.0898969\t Validation loss: 0.0938437\n",
      "\t Label value: 1.825057864189148\t Predicted Output: 1.7457321882247925\n",
      "[50] Training loss: 0.0864050\t Validation loss: 0.0902332\n",
      "\t Label value: 1.3731175661087036\t Predicted Output: 1.3530640602111816\n",
      "[51] Training loss: 0.0830811\t Validation loss: 0.0867881\n",
      "\t Label value: 3.6626856327056885\t Predicted Output: 3.425501823425293\n",
      "[52] Training loss: 0.0799015\t Validation loss: 0.0834211\n",
      "\t Label value: 3.1350510120391846\t Predicted Output: 3.074206590652466\n",
      "[53] Training loss: 0.0768180\t Validation loss: 0.0802250\n",
      "\t Label value: 1.7561604976654053\t Predicted Output: 1.6854161024093628\n",
      "[54] Training loss: 0.0738320\t Validation loss: 0.0771834\n",
      "\t Label value: -0.02923947386443615\t Predicted Output: 0.4630975127220154\n",
      "[55] Training loss: 0.0710046\t Validation loss: 0.0741394\n",
      "\t Label value: 3.7281107902526855\t Predicted Output: 3.740084171295166\n",
      "[56] Training loss: 0.0682979\t Validation loss: 0.0713340\n",
      "\t Label value: 3.7272772789001465\t Predicted Output: 3.7454099655151367\n",
      "[57] Training loss: 0.0656973\t Validation loss: 0.0685618\n",
      "\t Label value: 3.2650034427642822\t Predicted Output: 3.1697986125946045\n",
      "[58] Training loss: 0.0631946\t Validation loss: 0.0659245\n",
      "\t Label value: 3.2128987312316895\t Predicted Output: 3.150688886642456\n",
      "[59] Training loss: 0.0607668\t Validation loss: 0.0633694\n",
      "\t Label value: 2.4172041416168213\t Predicted Output: 2.348036766052246\n",
      "[60] Training loss: 0.0584633\t Validation loss: 0.0609359\n",
      "\t Label value: 3.4265339374542236\t Predicted Output: 3.9854063987731934\n",
      "[61] Training loss: 0.0562413\t Validation loss: 0.0586285\n",
      "\t Label value: 3.738877058029175\t Predicted Output: 3.545016050338745\n",
      "[62] Training loss: 0.0540992\t Validation loss: 0.0563935\n",
      "\t Label value: 3.406426429748535\t Predicted Output: 3.267683506011963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[63] Training loss: 0.0521185\t Validation loss: 0.0543151\n",
      "\t Label value: 3.674793004989624\t Predicted Output: 3.798083782196045\n",
      "[64] Training loss: 0.0501421\t Validation loss: 0.0523536\n",
      "\t Label value: 3.3045196533203125\t Predicted Output: 3.239467144012451\n",
      "[65] Training loss: 0.0483148\t Validation loss: 0.0502709\n",
      "\t Label value: 0.6492391228675842\t Predicted Output: 0.6790294647216797\n",
      "[66] Training loss: 0.0465210\t Validation loss: 0.0484103\n",
      "\t Label value: 2.886937379837036\t Predicted Output: 2.9042389392852783\n",
      "[67] Training loss: 0.0448168\t Validation loss: 0.0466323\n",
      "\t Label value: 3.7480459213256836\t Predicted Output: 3.671168565750122\n",
      "[68] Training loss: 0.0432311\t Validation loss: 0.0449760\n",
      "\t Label value: 2.9625179767608643\t Predicted Output: 3.00754451751709\n",
      "[69] Training loss: 0.0416902\t Validation loss: 0.0433824\n",
      "\t Label value: 3.262395143508911\t Predicted Output: 3.2372517585754395\n",
      "[70] Training loss: 0.0402159\t Validation loss: 0.0418837\n",
      "\t Label value: 0.7197855114936829\t Predicted Output: 0.7135360836982727\n",
      "[71] Training loss: 0.0388034\t Validation loss: 0.0403567\n",
      "\t Label value: 3.4683024883270264\t Predicted Output: 3.9095988273620605\n",
      "[72] Training loss: 0.0374443\t Validation loss: 0.0389179\n",
      "\t Label value: 3.5186922550201416\t Predicted Output: 3.3606276512145996\n",
      "[73] Training loss: 0.0361368\t Validation loss: 0.0375354\n",
      "\t Label value: 3.6751601696014404\t Predicted Output: 3.7612009048461914\n",
      "[74] Training loss: 0.0348659\t Validation loss: 0.0362145\n",
      "\t Label value: 2.8544294834136963\t Predicted Output: 2.883864641189575\n",
      "[75] Training loss: 0.0336636\t Validation loss: 0.0349305\n",
      "\t Label value: 1.1019821166992188\t Predicted Output: 1.0668832063674927\n",
      "[76] Training loss: 0.0324685\t Validation loss: 0.0336902\n",
      "\t Label value: 3.4572668075561523\t Predicted Output: 3.3412532806396484\n",
      "[77] Training loss: 0.0313408\t Validation loss: 0.0324982\n",
      "\t Label value: 2.6059207916259766\t Predicted Output: 2.595153570175171\n",
      "[78] Training loss: 0.0302445\t Validation loss: 0.0313340\n",
      "\t Label value: 1.4081088304519653\t Predicted Output: 1.3818572759628296\n",
      "[79] Training loss: 0.0291861\t Validation loss: 0.0302376\n",
      "\t Label value: 2.386291742324829\t Predicted Output: 2.3609628677368164\n",
      "[80] Training loss: 0.0281821\t Validation loss: 0.0291917\n",
      "\t Label value: 3.1024138927459717\t Predicted Output: 3.1774563789367676\n",
      "[81] Training loss: 0.0272016\t Validation loss: 0.0281190\n",
      "\t Label value: 0.3942074477672577\t Predicted Output: 0.41491907835006714\n",
      "[82] Training loss: 0.0262392\t Validation loss: 0.0271437\n",
      "\t Label value: -0.02923947386443615\t Predicted Output: 0.06497687101364136\n",
      "[83] Training loss: 0.0253226\t Validation loss: 0.0262192\n",
      "\t Label value: 1.3517683744430542\t Predicted Output: 1.3325592279434204\n",
      "[84] Training loss: 0.0244520\t Validation loss: 0.0252775\n",
      "\t Label value: 3.022974729537964\t Predicted Output: 3.0739331245422363\n",
      "[85] Training loss: 0.0235941\t Validation loss: 0.0243197\n",
      "\t Label value: 1.91094970703125\t Predicted Output: 1.9096076488494873\n",
      "[86] Training loss: 0.0227713\t Validation loss: 0.0234476\n",
      "\t Label value: 1.1019821166992188\t Predicted Output: 1.062327265739441\n",
      "[87] Training loss: 0.0219588\t Validation loss: 0.0226579\n",
      "\t Label value: 3.3591055870056152\t Predicted Output: 3.361588954925537\n",
      "[88] Training loss: 0.0212408\t Validation loss: 0.0218219\n",
      "\t Label value: 2.2928354740142822\t Predicted Output: 2.273688793182373\n",
      "[89] Training loss: 0.0204992\t Validation loss: 0.0210169\n",
      "\t Label value: 1.3464215993881226\t Predicted Output: 1.334593653678894\n",
      "[90] Training loss: 0.0197834\t Validation loss: 0.0202635\n",
      "\t Label value: 0.1456320732831955\t Predicted Output: 0.17441993951797485\n",
      "[91] Training loss: 0.0191037\t Validation loss: 0.0195226\n",
      "\t Label value: -0.5685386061668396\t Predicted Output: -0.06765967607498169\n",
      "[92] Training loss: 0.0184440\t Validation loss: 0.0188474\n",
      "\t Label value: 1.5936634540557861\t Predicted Output: 1.6282541751861572\n",
      "[93] Training loss: 0.0178201\t Validation loss: 0.0181622\n",
      "\t Label value: 1.8494644165039062\t Predicted Output: 1.8642631769180298\n",
      "[94] Training loss: 0.0171993\t Validation loss: 0.0175009\n",
      "\t Label value: 0.7717157602310181\t Predicted Output: 0.7409418821334839\n",
      "[95] Training loss: 0.0166183\t Validation loss: 0.0168834\n",
      "\t Label value: 1.0404638051986694\t Predicted Output: 1.0088497400283813\n",
      "[96] Training loss: 0.0160616\t Validation loss: 0.0162928\n",
      "\t Label value: 2.8978490829467773\t Predicted Output: 2.9066386222839355\n",
      "[97] Training loss: 0.0155132\t Validation loss: 0.0157235\n",
      "\t Label value: 2.586895227432251\t Predicted Output: 2.5851922035217285\n",
      "[98] Training loss: 0.0149745\t Validation loss: 0.0151445\n",
      "\t Label value: 0.39945805072784424\t Predicted Output: 0.39196380972862244\n",
      "[99] Training loss: 0.0144772\t Validation loss: 0.0146104\n",
      "\t Label value: 2.587907552719116\t Predicted Output: 2.583136796951294\n",
      "[100] Training loss: 0.0140133\t Validation loss: 0.0141033\n",
      "\t Label value: 3.6175191402435303\t Predicted Output: 3.734771251678467\n",
      "[101] Training loss: 0.0135388\t Validation loss: 0.0136408\n",
      "\t Label value: 3.3972861766815186\t Predicted Output: 3.417160749435425\n",
      "[102] Training loss: 0.0131118\t Validation loss: 0.0131495\n",
      "\t Label value: 2.6246109008789062\t Predicted Output: 2.613753080368042\n",
      "[103] Training loss: 0.0126611\t Validation loss: 0.0127508\n",
      "\t Label value: 3.0302481651306152\t Predicted Output: 3.048269510269165\n",
      "[104] Training loss: 0.0122483\t Validation loss: 0.0122673\n",
      "\t Label value: 3.437479257583618\t Predicted Output: 3.4469761848449707\n",
      "[105] Training loss: 0.0118636\t Validation loss: 0.0118376\n",
      "\t Label value: 3.755368232727051\t Predicted Output: 3.622314929962158\n",
      "[106] Training loss: 0.0114751\t Validation loss: 0.0114585\n",
      "\t Label value: 3.715794324874878\t Predicted Output: 3.563800096511841\n",
      "[107] Training loss: 0.0111160\t Validation loss: 0.0110672\n",
      "\t Label value: 0.6693910956382751\t Predicted Output: 0.6365102529525757\n",
      "[108] Training loss: 0.0107567\t Validation loss: 0.0107025\n",
      "\t Label value: 3.261514902114868\t Predicted Output: 3.3354508876800537\n",
      "[109] Training loss: 0.0104319\t Validation loss: 0.0103476\n",
      "\t Label value: 2.8783814907073975\t Predicted Output: 2.865208864212036\n",
      "[110] Training loss: 0.0100923\t Validation loss: 0.0100367\n",
      "\t Label value: 3.4336392879486084\t Predicted Output: 3.7611846923828125\n",
      "[111] Training loss: 0.0097859\t Validation loss: 0.0096804\n",
      "\t Label value: 3.519329071044922\t Predicted Output: 3.7294602394104004\n",
      "[112] Training loss: 0.0094811\t Validation loss: 0.0093662\n",
      "\t Label value: 3.7467501163482666\t Predicted Output: 3.6436927318573\n",
      "[113] Training loss: 0.0091818\t Validation loss: 0.0090966\n",
      "\t Label value: 0.14746907353401184\t Predicted Output: 0.1376429945230484\n",
      "[114] Training loss: 0.0089225\t Validation loss: 0.0087900\n",
      "\t Label value: 0.6693910956382751\t Predicted Output: 0.6429886817932129\n",
      "[115] Training loss: 0.0086481\t Validation loss: 0.0085123\n",
      "\t Label value: 3.603567600250244\t Predicted Output: 3.5501842498779297\n",
      "[116] Training loss: 0.0083953\t Validation loss: 0.0082466\n",
      "\t Label value: 0.6011781096458435\t Predicted Output: 0.5724855661392212\n",
      "[117] Training loss: 0.0081404\t Validation loss: 0.0079951\n",
      "\t Label value: 0.01630239002406597\t Predicted Output: 0.015474572777748108\n",
      "[118] Training loss: 0.0078998\t Validation loss: 0.0077741\n",
      "\t Label value: 2.8029892444610596\t Predicted Output: 2.782527208328247\n",
      "[119] Training loss: 0.0076717\t Validation loss: 0.0075140\n",
      "\t Label value: 3.5318803787231445\t Predicted Output: 3.5338096618652344\n",
      "[120] Training loss: 0.0074529\t Validation loss: 0.0072996\n",
      "\t Label value: 0.003892510896548629\t Predicted Output: 0.01025301218032837\n",
      "[121] Training loss: 0.0072150\t Validation loss: 0.0070730\n",
      "\t Label value: 2.7942612171173096\t Predicted Output: 2.767681121826172\n",
      "[122] Training loss: 0.0070154\t Validation loss: 0.0068728\n",
      "\t Label value: 1.3327410221099854\t Predicted Output: 1.3486268520355225\n",
      "[123] Training loss: 0.0067967\t Validation loss: 0.0066649\n",
      "\t Label value: 2.6364026069641113\t Predicted Output: 2.6154625415802\n",
      "[124] Training loss: 0.0066132\t Validation loss: 0.0064791\n",
      "\t Label value: 3.6838672161102295\t Predicted Output: 3.6074023246765137\n",
      "[125] Training loss: 0.0064142\t Validation loss: 0.0063049\n",
      "\t Label value: 3.251265048980713\t Predicted Output: 3.299731969833374\n",
      "[126] Training loss: 0.0062478\t Validation loss: 0.0060876\n",
      "\t Label value: 3.69770884513855\t Predicted Output: 3.6625516414642334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[127] Training loss: 0.0060689\t Validation loss: 0.0059152\n",
      "\t Label value: 0.8791296482086182\t Predicted Output: 0.8538746237754822\n",
      "[128] Training loss: 0.0058879\t Validation loss: 0.0057544\n",
      "\t Label value: 3.4173877239227295\t Predicted Output: 3.6880085468292236\n",
      "[129] Training loss: 0.0057316\t Validation loss: 0.0055873\n",
      "\t Label value: 3.5231566429138184\t Predicted Output: 3.5437488555908203\n",
      "[130] Training loss: 0.0055685\t Validation loss: 0.0054279\n",
      "\t Label value: 2.3686416149139404\t Predicted Output: 2.356760025024414\n",
      "[131] Training loss: 0.0054092\t Validation loss: 0.0052734\n",
      "\t Label value: 3.719970464706421\t Predicted Output: 3.657829999923706\n",
      "[132] Training loss: 0.0052549\t Validation loss: 0.0051240\n",
      "\t Label value: 3.7338521480560303\t Predicted Output: 3.651557683944702\n",
      "[133] Training loss: 0.0051090\t Validation loss: 0.0049804\n",
      "\t Label value: 2.9556994438171387\t Predicted Output: 2.9380807876586914\n",
      "[134] Training loss: 0.0049662\t Validation loss: 0.0048377\n",
      "\t Label value: 2.256880283355713\t Predicted Output: 2.2472875118255615\n",
      "[135] Training loss: 0.0048263\t Validation loss: 0.0047179\n",
      "\t Label value: 2.3934895992279053\t Predicted Output: 2.3842966556549072\n",
      "[136] Training loss: 0.0046755\t Validation loss: 0.0045627\n",
      "\t Label value: 1.5524685382843018\t Predicted Output: 1.593405842781067\n",
      "[137] Training loss: 0.0045543\t Validation loss: 0.0044379\n",
      "\t Label value: 3.341473340988159\t Predicted Output: 3.363687753677368\n",
      "[138] Training loss: 0.0044277\t Validation loss: 0.0043148\n",
      "\t Label value: 0.4294224679470062\t Predicted Output: 0.4022374451160431\n",
      "[139] Training loss: 0.0043067\t Validation loss: 0.0041889\n",
      "\t Label value: 3.735377073287964\t Predicted Output: 3.648190498352051\n",
      "[140] Training loss: 0.0041831\t Validation loss: 0.0040953\n",
      "\t Label value: -0.07760455459356308\t Predicted Output: -0.07383978366851807\n",
      "[141] Training loss: 0.0040616\t Validation loss: 0.0039884\n",
      "\t Label value: 3.450934648513794\t Predicted Output: 3.4932196140289307\n",
      "[142] Training loss: 0.0039681\t Validation loss: 0.0038607\n",
      "\t Label value: 2.304396152496338\t Predicted Output: 2.2973744869232178\n",
      "[143] Training loss: 0.0038431\t Validation loss: 0.0037787\n",
      "\t Label value: 3.4433536529541016\t Predicted Output: 3.643362522125244\n",
      "[144] Training loss: 0.0037602\t Validation loss: 0.0036644\n",
      "\t Label value: 3.3534116744995117\t Predicted Output: 3.359726667404175\n",
      "[145] Training loss: 0.0036472\t Validation loss: 0.0035779\n",
      "\t Label value: 0.01630239002406597\t Predicted Output: 0.012397393584251404\n",
      "[146] Training loss: 0.0035633\t Validation loss: 0.0035328\n",
      "\t Label value: 1.0840562582015991\t Predicted Output: 1.0679023265838623\n",
      "[147] Training loss: 0.0034803\t Validation loss: 0.0033999\n",
      "\t Label value: 3.7547950744628906\t Predicted Output: 3.6574554443359375\n",
      "[148] Training loss: 0.0033920\t Validation loss: 0.0033211\n",
      "\t Label value: 2.3674111366271973\t Predicted Output: 2.3584437370300293\n",
      "[149] Training loss: 0.0033126\t Validation loss: 0.0032582\n",
      "\t Label value: 3.736017942428589\t Predicted Output: 3.6744086742401123\n",
      "[150] Training loss: 0.0032254\t Validation loss: 0.0031850\n",
      "\t Label value: 1.8258049488067627\t Predicted Output: 1.8825188875198364\n",
      "[151] Training loss: 0.0031586\t Validation loss: 0.0031199\n",
      "\t Label value: 3.5794994831085205\t Predicted Output: 3.645894765853882\n",
      "[152] Training loss: 0.0030896\t Validation loss: 0.0030229\n",
      "\t Label value: 3.6959545612335205\t Predicted Output: 3.650190591812134\n",
      "[153] Training loss: 0.0030192\t Validation loss: 0.0029635\n",
      "\t Label value: 2.3003275394439697\t Predicted Output: 2.2972333431243896\n",
      "[154] Training loss: 0.0029401\t Validation loss: 0.0029100\n",
      "\t Label value: 3.674793004989624\t Predicted Output: 3.6525285243988037\n",
      "[155] Training loss: 0.0028804\t Validation loss: 0.0028755\n",
      "\t Label value: 3.607355833053589\t Predicted Output: 3.635610580444336\n",
      "[156] Training loss: 0.0028200\t Validation loss: 0.0027613\n",
      "\t Label value: 3.435077428817749\t Predicted Output: 3.620109796524048\n",
      "[157] Training loss: 0.0027495\t Validation loss: 0.0027058\n",
      "\t Label value: 3.752840757369995\t Predicted Output: 3.669283866882324\n",
      "[158] Training loss: 0.0026904\t Validation loss: 0.0026470\n",
      "\t Label value: 2.3496932983398438\t Predicted Output: 2.341381311416626\n",
      "[159] Training loss: 0.0026268\t Validation loss: 0.0025954\n",
      "\t Label value: 3.685502767562866\t Predicted Output: 3.63968825340271\n",
      "[160] Training loss: 0.0025676\t Validation loss: 0.0025275\n",
      "\t Label value: 3.74204158782959\t Predicted Output: 3.689485788345337\n",
      "[161] Training loss: 0.0025097\t Validation loss: 0.0024846\n",
      "\t Label value: 1.4416941404342651\t Predicted Output: 1.464608907699585\n",
      "[162] Training loss: 0.0024585\t Validation loss: 0.0024175\n",
      "\t Label value: 3.076361656188965\t Predicted Output: 3.0690653324127197\n",
      "[163] Training loss: 0.0024000\t Validation loss: 0.0023870\n",
      "\t Label value: 3.615835189819336\t Predicted Output: 3.637159585952759\n",
      "[164] Training loss: 0.0023439\t Validation loss: 0.0023336\n",
      "\t Label value: -0.2358575165271759\t Predicted Output: -0.21040789783000946\n",
      "[165] Training loss: 0.0022875\t Validation loss: 0.0022861\n",
      "\t Label value: 3.408743381500244\t Predicted Output: 3.4235739707946777\n",
      "[166] Training loss: 0.0022412\t Validation loss: 0.0022514\n",
      "\t Label value: 2.650172233581543\t Predicted Output: 2.6347975730895996\n",
      "[167] Training loss: 0.0021964\t Validation loss: 0.0021819\n",
      "\t Label value: 2.842139482498169\t Predicted Output: 2.8396480083465576\n",
      "[168] Training loss: 0.0021478\t Validation loss: 0.0021312\n",
      "\t Label value: -0.02923947386443615\t Predicted Output: -0.02984410524368286\n",
      "[169] Training loss: 0.0020987\t Validation loss: 0.0020960\n",
      "\t Label value: 0.022232720628380775\t Predicted Output: 0.018854573369026184\n",
      "[170] Training loss: 0.0020528\t Validation loss: 0.0020379\n",
      "\t Label value: 3.753541946411133\t Predicted Output: 3.6948697566986084\n",
      "[171] Training loss: 0.0020115\t Validation loss: 0.0019979\n",
      "\t Label value: 0.04294518008828163\t Predicted Output: 0.0350877046585083\n",
      "[172] Training loss: 0.0019636\t Validation loss: 0.0019525\n",
      "\t Label value: 3.6349170207977295\t Predicted Output: 3.652890205383301\n",
      "[173] Training loss: 0.0019294\t Validation loss: 0.0019184\n",
      "\t Label value: 3.511657476425171\t Predicted Output: 3.5110487937927246\n",
      "[174] Training loss: 0.0018900\t Validation loss: 0.0018785\n",
      "\t Label value: 1.9856112003326416\t Predicted Output: 2.020303964614868\n",
      "[175] Training loss: 0.0018455\t Validation loss: 0.0018397\n",
      "\t Label value: 3.7334415912628174\t Predicted Output: 3.6660449504852295\n",
      "[176] Training loss: 0.0018167\t Validation loss: 0.0018108\n",
      "\t Label value: 3.5735018253326416\t Predicted Output: 3.614222288131714\n",
      "[177] Training loss: 0.0017783\t Validation loss: 0.0017801\n",
      "\t Label value: 3.399235725402832\t Predicted Output: 3.402508020401001\n",
      "[178] Training loss: 0.0017424\t Validation loss: 0.0017702\n",
      "\t Label value: 3.745222568511963\t Predicted Output: 3.7263171672821045\n",
      "[179] Training loss: 0.0017130\t Validation loss: 0.0017161\n",
      "\t Label value: 3.2422704696655273\t Predicted Output: 3.2347264289855957\n",
      "[180] Training loss: 0.0016830\t Validation loss: 0.0016836\n",
      "\t Label value: 3.3659300804138184\t Predicted Output: 3.372894048690796\n",
      "[181] Training loss: 0.0016479\t Validation loss: 0.0016547\n",
      "\t Label value: 1.3274731636047363\t Predicted Output: 1.3331692218780518\n",
      "[182] Training loss: 0.0016225\t Validation loss: 0.0016182\n",
      "\t Label value: 3.6555628776550293\t Predicted Output: 3.6775310039520264\n",
      "[183] Training loss: 0.0015984\t Validation loss: 0.0016020\n",
      "\t Label value: 3.5176548957824707\t Predicted Output: 3.593829393386841\n",
      "[184] Training loss: 0.0015611\t Validation loss: 0.0015636\n",
      "\t Label value: 3.6557464599609375\t Predicted Output: 3.6770308017730713\n",
      "[185] Training loss: 0.0015418\t Validation loss: 0.0015399\n",
      "\t Label value: 3.5003483295440674\t Predicted Output: 3.4918296337127686\n",
      "[186] Training loss: 0.0015060\t Validation loss: 0.0015767\n",
      "\t Label value: 3.612929582595825\t Predicted Output: 3.6256699562072754\n",
      "[187] Training loss: 0.0014936\t Validation loss: 0.0014881\n",
      "\t Label value: 3.755521297454834\t Predicted Output: 3.707977056503296\n",
      "[188] Training loss: 0.0014633\t Validation loss: 0.0014633\n",
      "\t Label value: 3.639357566833496\t Predicted Output: 3.6532950401306152\n",
      "[189] Training loss: 0.0014407\t Validation loss: 0.0014386\n",
      "\t Label value: 1.1037230491638184\t Predicted Output: 1.0833053588867188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[190] Training loss: 0.0014173\t Validation loss: 0.0014241\n",
      "\t Label value: 2.710103750228882\t Predicted Output: 2.6971709728240967\n",
      "[191] Training loss: 0.0013941\t Validation loss: 0.0013942\n",
      "\t Label value: 3.548072099685669\t Predicted Output: 3.542182207107544\n",
      "[192] Training loss: 0.0013661\t Validation loss: 0.0013804\n",
      "\t Label value: 0.1456320732831955\t Predicted Output: 0.1283889263868332\n",
      "[193] Training loss: 0.0013463\t Validation loss: 0.0013535\n",
      "\t Label value: 3.6091506481170654\t Predicted Output: 3.612236738204956\n",
      "[194] Training loss: 0.0013265\t Validation loss: 0.0013291\n",
      "\t Label value: 2.3003275394439697\t Predicted Output: 2.289621114730835\n",
      "[195] Training loss: 0.0013024\t Validation loss: 0.0013102\n",
      "\t Label value: 3.4698431491851807\t Predicted Output: 3.562011241912842\n",
      "[196] Training loss: 0.0012832\t Validation loss: 0.0012982\n",
      "\t Label value: 3.636927604675293\t Predicted Output: 3.6411292552948\n",
      "[197] Training loss: 0.0012646\t Validation loss: 0.0012808\n",
      "\t Label value: 3.6617400646209717\t Predicted Output: 3.634204626083374\n",
      "[198] Training loss: 0.0012469\t Validation loss: 0.0012537\n",
      "\t Label value: 1.559402346611023\t Predicted Output: 1.6002557277679443\n",
      "[199] Training loss: 0.0012210\t Validation loss: 0.0012368\n",
      "\t Label value: 3.591641902923584\t Predicted Output: 3.589202642440796\n",
      "[200] Training loss: 0.0012071\t Validation loss: 0.0012492\n",
      "\t Label value: 3.7155752182006836\t Predicted Output: 3.7386882305145264\n",
      "[201] Training loss: 0.0011903\t Validation loss: 0.0011981\n",
      "\t Label value: 1.5798659324645996\t Predicted Output: 1.6256541013717651\n",
      "[202] Training loss: 0.0011712\t Validation loss: 0.0011833\n",
      "\t Label value: 0.6099777221679688\t Predicted Output: 0.5721535086631775\n",
      "[203] Training loss: 0.0011527\t Validation loss: 0.0011694\n",
      "\t Label value: 2.787977457046509\t Predicted Output: 2.7775259017944336\n",
      "[204] Training loss: 0.0011345\t Validation loss: 0.0011490\n",
      "\t Label value: 3.7550761699676514\t Predicted Output: 3.733253002166748\n",
      "[205] Training loss: 0.0011202\t Validation loss: 0.0011869\n",
      "\t Label value: 3.207153081893921\t Predicted Output: 3.1968722343444824\n",
      "[206] Training loss: 0.0011062\t Validation loss: 0.0011214\n",
      "\t Label value: 3.4966816902160645\t Predicted Output: 3.5649237632751465\n",
      "[207] Training loss: 0.0010835\t Validation loss: 0.0011140\n",
      "\t Label value: 3.746260404586792\t Predicted Output: 3.768831968307495\n",
      "[208] Training loss: 0.0010723\t Validation loss: 0.0010885\n",
      "\t Label value: 3.7543108463287354\t Predicted Output: 3.7450649738311768\n",
      "[209] Training loss: 0.0010571\t Validation loss: 0.0010856\n",
      "\t Label value: 3.7183191776275635\t Predicted Output: 3.7346084117889404\n",
      "[210] Training loss: 0.0010439\t Validation loss: 0.0010584\n",
      "\t Label value: 3.426237106323242\t Predicted Output: 3.5333218574523926\n",
      "[211] Training loss: 0.0010258\t Validation loss: 0.0010436\n",
      "\t Label value: 3.7428462505340576\t Predicted Output: 3.7706785202026367\n",
      "[212] Training loss: 0.0010153\t Validation loss: 0.0010344\n",
      "\t Label value: 3.678788661956787\t Predicted Output: 3.6368789672851562\n",
      "[213] Training loss: 0.0010008\t Validation loss: 0.0010154\n",
      "\t Label value: 3.406426429748535\t Predicted Output: 3.4159672260284424\n",
      "[214] Training loss: 0.0009862\t Validation loss: 0.0010036\n",
      "\t Label value: 3.656052827835083\t Predicted Output: 3.627068281173706\n",
      "[215] Training loss: 0.0009711\t Validation loss: 0.0009931\n",
      "\t Label value: 3.4243385791778564\t Predicted Output: 3.52734112739563\n",
      "[216] Training loss: 0.0009583\t Validation loss: 0.0009792\n",
      "\t Label value: 3.6997740268707275\t Predicted Output: 3.7077407836914062\n",
      "[217] Training loss: 0.0009468\t Validation loss: 0.0009661\n",
      "\t Label value: 3.452615976333618\t Predicted Output: 3.5397512912750244\n",
      "[218] Training loss: 0.0009354\t Validation loss: 0.0009560\n",
      "\t Label value: 1.0366960763931274\t Predicted Output: 1.01566481590271\n",
      "[219] Training loss: 0.0009223\t Validation loss: 0.0009828\n",
      "\t Label value: 3.7028846740722656\t Predicted Output: 3.7133383750915527\n",
      "[220] Training loss: 0.0009108\t Validation loss: 0.0009284\n",
      "\t Label value: 3.261514902114868\t Predicted Output: 3.2570369243621826\n",
      "[221] Training loss: 0.0008974\t Validation loss: 0.0009198\n",
      "\t Label value: 3.613381862640381\t Predicted Output: 3.600186347961426\n",
      "[222] Training loss: 0.0008837\t Validation loss: 0.0009165\n",
      "\t Label value: 3.6805331707000732\t Predicted Output: 3.6360743045806885\n",
      "[223] Training loss: 0.0008742\t Validation loss: 0.0008958\n",
      "\t Label value: 0.3782874345779419\t Predicted Output: 0.3448851704597473\n",
      "[224] Training loss: 0.0008655\t Validation loss: 0.0008916\n",
      "\t Label value: 3.7422707080841064\t Predicted Output: 3.756537437438965\n",
      "[225] Training loss: 0.0008542\t Validation loss: 0.0008847\n",
      "\t Label value: 2.232469320297241\t Predicted Output: 2.2265682220458984\n",
      "[226] Training loss: 0.0008415\t Validation loss: 0.0008665\n",
      "\t Label value: 3.707310914993286\t Predicted Output: 3.7082762718200684\n",
      "[227] Training loss: 0.0008324\t Validation loss: 0.0008565\n",
      "\t Label value: 3.607355833053589\t Predicted Output: 3.6021218299865723\n",
      "[228] Training loss: 0.0008228\t Validation loss: 0.0008444\n",
      "\t Label value: 3.7437126636505127\t Predicted Output: 3.7093358039855957\n",
      "[229] Training loss: 0.0008161\t Validation loss: 0.0008395\n",
      "\t Label value: 3.350813150405884\t Predicted Output: 3.3513729572296143\n",
      "[230] Training loss: 0.0008047\t Validation loss: 0.0008345\n",
      "\t Label value: 3.715794324874878\t Predicted Output: 3.7220804691314697\n",
      "[231] Training loss: 0.0007918\t Validation loss: 0.0008452\n",
      "\t Label value: 3.755521297454834\t Predicted Output: 3.7539925575256348\n",
      "[232] Training loss: 0.0007886\t Validation loss: 0.0008117\n",
      "\t Label value: 3.4332435131073\t Predicted Output: 3.4470067024230957\n",
      "[233] Training loss: 0.0007764\t Validation loss: 0.0008023\n",
      "\t Label value: 3.472228765487671\t Predicted Output: 3.5319266319274902\n",
      "[234] Training loss: 0.0007720\t Validation loss: 0.0008040\n",
      "\t Label value: 3.400315999984741\t Predicted Output: 3.507727861404419\n",
      "[235] Training loss: 0.0007635\t Validation loss: 0.0007961\n",
      "\t Label value: -0.18879352509975433\t Predicted Output: -0.17299145460128784\n",
      "[236] Training loss: 0.0007571\t Validation loss: 0.0007864\n",
      "\t Label value: 3.650909900665283\t Predicted Output: 3.6520349979400635\n",
      "[237] Training loss: 0.0007457\t Validation loss: 0.0008070\n",
      "\t Label value: 3.6882927417755127\t Predicted Output: 3.6541874408721924\n",
      "[238] Training loss: 0.0007423\t Validation loss: 0.0007879\n",
      "\t Label value: 3.463961362838745\t Predicted Output: 3.531783103942871\n",
      "[239] Training loss: 0.0007339\t Validation loss: 0.0007588\n",
      "\t Label value: 1.978015661239624\t Predicted Output: 2.0025296211242676\n",
      "[240] Training loss: 0.0007279\t Validation loss: 0.0007507\n",
      "\t Label value: 3.5931601524353027\t Predicted Output: 3.587754249572754\n",
      "[241] Training loss: 0.0007203\t Validation loss: 0.0007433\n",
      "\t Label value: 3.0575664043426514\t Predicted Output: 3.06404972076416\n",
      "[242] Training loss: 0.0007121\t Validation loss: 0.0007387\n",
      "\t Label value: 3.424741268157959\t Predicted Output: 3.436594009399414\n",
      "[243] Training loss: 0.0007050\t Validation loss: 0.0007480\n",
      "\t Label value: 3.557034492492676\t Predicted Output: 3.5528950691223145\n",
      "[244] Training loss: 0.0007029\t Validation loss: 0.0007482\n",
      "\t Label value: 2.7620849609375\t Predicted Output: 2.7532241344451904\n",
      "[245] Training loss: 0.0006931\t Validation loss: 0.0007212\n",
      "\t Label value: 3.2223682403564453\t Predicted Output: 3.2161335945129395\n",
      "[246] Training loss: 0.0006889\t Validation loss: 0.0007126\n",
      "\t Label value: 1.7944421768188477\t Predicted Output: 1.8474102020263672\n",
      "[247] Training loss: 0.0006812\t Validation loss: 0.0007036\n",
      "\t Label value: 3.6091506481170654\t Predicted Output: 3.6038637161254883\n",
      "[248] Training loss: 0.0006748\t Validation loss: 0.0007039\n",
      "\t Label value: 3.4433536529541016\t Predicted Output: 3.509605646133423\n",
      "[249] Training loss: 0.0006719\t Validation loss: 0.0006992\n",
      "\t Label value: 0.7824543714523315\t Predicted Output: 0.7532875537872314\n",
      "[250] Training loss: 0.0006642\t Validation loss: 0.0006868\n",
      "\t Label value: 3.7469518184661865\t Predicted Output: 3.7616262435913086\n",
      "[251] Training loss: 0.0006634\t Validation loss: 0.0006805\n",
      "\t Label value: 3.704533576965332\t Predicted Output: 3.7024810314178467\n",
      "[252] Training loss: 0.0006544\t Validation loss: 0.0006836\n",
      "\t Label value: 3.4524953365325928\t Predicted Output: 3.511945962905884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[253] Training loss: 0.0006516\t Validation loss: 0.0006758\n",
      "\t Label value: -0.2130238264799118\t Predicted Output: -0.1972765326499939\n",
      "[254] Training loss: 0.0006426\t Validation loss: 0.0006726\n",
      "\t Label value: 2.274501323699951\t Predicted Output: 2.2609832286834717\n",
      "[255] Training loss: 0.0006395\t Validation loss: 0.0006591\n",
      "\t Label value: 2.694017171859741\t Predicted Output: 2.67587947845459\n",
      "[256] Training loss: 0.0006334\t Validation loss: 0.0006602\n",
      "\t Label value: 0.6747041344642639\t Predicted Output: 0.6412810683250427\n",
      "[257] Training loss: 0.0006271\t Validation loss: 0.0006678\n",
      "\t Label value: 3.752633571624756\t Predicted Output: 3.7571399211883545\n",
      "[258] Training loss: 0.0006265\t Validation loss: 0.0006454\n",
      "\t Label value: 3.611250162124634\t Predicted Output: 3.594015121459961\n",
      "[259] Training loss: 0.0006172\t Validation loss: 0.0006416\n",
      "\t Label value: 3.457643508911133\t Predicted Output: 3.510622978210449\n",
      "[260] Training loss: 0.0006132\t Validation loss: 0.0006371\n",
      "\t Label value: 1.6114323139190674\t Predicted Output: 1.674578070640564\n",
      "[261] Training loss: 0.0006101\t Validation loss: 0.0006367\n",
      "\t Label value: 2.4554483890533447\t Predicted Output: 2.4319660663604736\n",
      "[262] Training loss: 0.0006026\t Validation loss: 0.0006306\n",
      "\t Label value: 0.20165535807609558\t Predicted Output: 0.1824776977300644\n",
      "[263] Training loss: 0.0006013\t Validation loss: 0.0006212\n",
      "\t Label value: 3.3407950401306152\t Predicted Output: 3.3376340866088867\n",
      "[264] Training loss: 0.0005980\t Validation loss: 0.0006193\n",
      "\t Label value: 0.5935770273208618\t Predicted Output: 0.5593717694282532\n",
      "[265] Training loss: 0.0005919\t Validation loss: 0.0006265\n",
      "\t Label value: 2.3934895992279053\t Predicted Output: 2.373976469039917\n",
      "[266] Training loss: 0.0005892\t Validation loss: 0.0006087\n",
      "\t Label value: 2.46282696723938\t Predicted Output: 2.438498020172119\n",
      "[267] Training loss: 0.0005853\t Validation loss: 0.0006029\n",
      "\t Label value: 3.7428462505340576\t Predicted Output: 3.743514060974121\n",
      "[268] Training loss: 0.0005793\t Validation loss: 0.0006143\n",
      "\t Label value: 3.69770884513855\t Predicted Output: 3.6692054271698\n",
      "[269] Training loss: 0.0005752\t Validation loss: 0.0006016\n",
      "\t Label value: 3.4906656742095947\t Predicted Output: 3.4944498538970947\n",
      "[270] Training loss: 0.0005705\t Validation loss: 0.0005957\n",
      "\t Label value: 2.7826743125915527\t Predicted Output: 2.7731339931488037\n",
      "[271] Training loss: 0.0005683\t Validation loss: 0.0005916\n",
      "\t Label value: 3.6701595783233643\t Predicted Output: 3.6706080436706543\n",
      "[272] Training loss: 0.0005636\t Validation loss: 0.0005884\n",
      "\t Label value: 0.21474532783031464\t Predicted Output: 0.19427983462810516\n",
      "[273] Training loss: 0.0005615\t Validation loss: 0.0005819\n",
      "\t Label value: 1.9526190757751465\t Predicted Output: 1.9756238460540771\n",
      "[274] Training loss: 0.0005567\t Validation loss: 0.0005770\n",
      "\t Label value: 2.2164764404296875\t Predicted Output: 2.20564603805542\n",
      "[275] Training loss: 0.0005531\t Validation loss: 0.0005749\n",
      "\t Label value: 3.7359061241149902\t Predicted Output: 3.732924222946167\n",
      "[276] Training loss: 0.0005489\t Validation loss: 0.0005708\n",
      "\t Label value: 1.8065179586410522\t Predicted Output: 1.8517208099365234\n",
      "[277] Training loss: 0.0005430\t Validation loss: 0.0005979\n",
      "\t Label value: 3.464508056640625\t Predicted Output: 3.4993679523468018\n",
      "[278] Training loss: 0.0005415\t Validation loss: 0.0005637\n",
      "\t Label value: 3.753113269805908\t Predicted Output: 3.771008014678955\n",
      "[279] Training loss: 0.0005364\t Validation loss: 0.0005705\n",
      "\t Label value: 3.6312928199768066\t Predicted Output: 3.6297667026519775\n",
      "[280] Training loss: 0.0005346\t Validation loss: 0.0005578\n",
      "\t Label value: -0.30009663105010986\t Predicted Output: -0.2781282663345337\n",
      "[281] Training loss: 0.0005282\t Validation loss: 0.0005571\n",
      "\t Label value: 2.586895227432251\t Predicted Output: 2.5646629333496094\n",
      "[282] Training loss: 0.0005257\t Validation loss: 0.0005547\n",
      "\t Label value: 3.7550859451293945\t Predicted Output: 3.776066541671753\n",
      "[283] Training loss: 0.0005227\t Validation loss: 0.0005483\n",
      "\t Label value: 3.6628942489624023\t Predicted Output: 3.6347038745880127\n",
      "[284] Training loss: 0.0005150\t Validation loss: 0.0005523\n",
      "\t Label value: 3.6679251194000244\t Predicted Output: 3.6735756397247314\n",
      "[285] Training loss: 0.0005160\t Validation loss: 0.0005519\n",
      "\t Label value: 3.3659300804138184\t Predicted Output: 3.3661301136016846\n",
      "[286] Training loss: 0.0005132\t Validation loss: 0.0005467\n",
      "\t Label value: 1.2446321249008179\t Predicted Output: 1.25397527217865\n",
      "[287] Training loss: 0.0005093\t Validation loss: 0.0005402\n",
      "\t Label value: 3.249783754348755\t Predicted Output: 3.241790533065796\n",
      "[288] Training loss: 0.0005068\t Validation loss: 0.0005450\n",
      "\t Label value: 1.8065179586410522\t Predicted Output: 1.8515381813049316\n",
      "[289] Training loss: 0.0005031\t Validation loss: 0.0005295\n",
      "\t Label value: 3.011526107788086\t Predicted Output: 3.014463186264038\n",
      "[290] Training loss: 0.0005006\t Validation loss: 0.0005275\n",
      "\t Label value: 2.3819820880889893\t Predicted Output: 2.3590333461761475\n",
      "[291] Training loss: 0.0004980\t Validation loss: 0.0005245\n",
      "\t Label value: 2.830697774887085\t Predicted Output: 2.8308043479919434\n",
      "[292] Training loss: 0.0004939\t Validation loss: 0.0005255\n",
      "\t Label value: 3.400315999984741\t Predicted Output: 3.469083786010742\n",
      "[293] Training loss: 0.0004903\t Validation loss: 0.0005193\n",
      "\t Label value: 3.755485773086548\t Predicted Output: 3.77304744720459\n",
      "[294] Training loss: 0.0004883\t Validation loss: 0.0005193\n",
      "\t Label value: 3.5681471824645996\t Predicted Output: 3.563901424407959\n",
      "[295] Training loss: 0.0004852\t Validation loss: 0.0005333\n",
      "\t Label value: 3.433911085128784\t Predicted Output: 3.4888784885406494\n",
      "[296] Training loss: 0.0004838\t Validation loss: 0.0005125\n",
      "\t Label value: 1.6061537265777588\t Predicted Output: 1.6752893924713135\n",
      "[297] Training loss: 0.0004794\t Validation loss: 0.0005137\n",
      "\t Label value: 2.502070665359497\t Predicted Output: 2.4780256748199463\n",
      "[298] Training loss: 0.0004771\t Validation loss: 0.0005059\n",
      "\t Label value: 3.7462637424468994\t Predicted Output: 3.755659818649292\n",
      "[299] Training loss: 0.0004738\t Validation loss: 0.0005091\n",
      "\t Label value: 1.743361473083496\t Predicted Output: 1.7954471111297607\n",
      "[300] Training loss: 0.0004710\t Validation loss: 0.0005081\n",
      "\t Label value: 3.710012912750244\t Predicted Output: 3.7001328468322754\n"
     ]
    }
   ],
   "source": [
    "model = Test_MLP(1, 8, 1)\n",
    "print(model)\n",
    "\n",
    "epochs = 300\n",
    "optimizer  = optim.Adam(model.parameters(), lr=1e-05)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=20, eta_min=1e-05)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "mf_dict =train_model(train_loader, val_loader, epochs, model, optimizer, scheduler, criterion)\n",
    "#mf_dict.keys().contains(\"bias\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.weight\n",
      "tensor([-0.4296,  0.0791, -0.2075,  1.1229, -0.8824, -1.1717,  0.0549,  0.3695])\n",
      "torch.Size([8])\n",
      "fc2.weight\n",
      "tensor([ 0.1273,  0.1462,  0.2124,  0.5475,  0.1835, -0.2722,  0.1423, -0.2814,\n",
      "        -0.4968,  0.3224, -0.2453, -0.2873, -0.2079, -0.5478, -0.3082,  0.4633,\n",
      "        -0.2312,  0.1844, -0.0031,  0.1777, -0.3513, -0.2784, -0.3505,  0.1783,\n",
      "        -0.1864, -0.1478,  0.2134, -0.0352, -0.2567, -0.3392, -0.1517,  0.0682,\n",
      "        -0.2250,  0.1092,  0.1293, -0.3273, -0.2466, -0.2939,  0.2976, -0.2350,\n",
      "        -0.4759,  0.6274,  0.2777,  0.2897, -0.4809, -0.1402, -0.3190,  0.2209,\n",
      "         0.0618, -0.0276,  0.1939,  0.4144,  0.2115, -0.2220,  0.2850, -0.0264])\n",
      "torch.Size([56])\n",
      "fc3.weight\n",
      "tensor([ 0.1614, -0.5984, -0.5499, -0.1632,  0.1903, -0.7242,  0.3013, -0.3459,\n",
      "         0.7090,  0.1920, -0.0781,  0.1575,  0.2099, -0.5087, -0.4621,  0.7770,\n",
      "         0.0019, -0.0109,  0.0486,  0.1476, -0.6907,  0.3182, -0.1687, -0.2444,\n",
      "        -0.3658, -0.1470, -0.3790, -0.3586, -0.1509,  0.6602,  0.4256, -0.3291,\n",
      "        -0.3190, -0.0140, -0.2290, -0.3634,  0.1511, -0.2037, -0.3068, -0.1814,\n",
      "        -0.2976,  0.1550, -0.3739,  0.4535,  0.6687, -0.0447,  0.0456,  0.1870,\n",
      "        -0.6953, -0.4239,  0.4850,  0.6361, -0.3624,  0.1563,  0.6032, -0.6278])\n",
      "torch.Size([56])\n",
      "fc4.weight\n",
      "tensor([-0.9025,  0.4633,  0.6140,  0.1989,  0.2202,  0.0842,  0.5652,  0.5338])\n",
      "torch.Size([8])\n",
      "\n",
      "[-0.42959514260292053, 0.07913845777511597, -0.2074979543685913, 1.1229357719421387, -0.8823912143707275, -1.1716675758361816, 0.05494260787963867, 0.3695339858531952, 0.12730225920677185, 0.14615464210510254, 0.21239319443702698, 0.5475261807441711, 0.18347962200641632, -0.2722232937812805, 0.1422681212425232, -0.28137242794036865, -0.4967779815196991, 0.32238146662712097, -0.24531884491443634, -0.28734198212623596, -0.20790964365005493, -0.5477886199951172, -0.308155357837677, 0.46328145265579224, -0.23116493225097656, 0.18437688052654266, -0.0031412839889526367, 0.17766369879245758, -0.3513280749320984, -0.27841442823410034, -0.350450336933136, 0.17833904922008514, -0.18638452887535095, -0.14784418046474457, 0.21335169672966003, -0.03520798683166504, -0.2566867768764496, -0.3392130732536316, -0.15167061984539032, 0.06820890307426453, -0.22495092451572418, 0.10923910140991211, 0.12934556603431702, -0.327303946018219, -0.24663926661014557, -0.2938797175884247, 0.297558456659317, -0.23500335216522217, -0.4759097695350647, 0.6274418830871582, 0.27768269181251526, 0.2896575629711151, -0.48087239265441895, -0.1402154564857483, -0.3189519941806793, 0.2209118753671646, 0.061761606484651566, -0.02762601338326931, 0.19385811686515808, 0.4143691956996918, 0.211472287774086, -0.22203697264194489, 0.28495076298713684, -0.026357321068644524, 0.16144584119319916, -0.5983656048774719, -0.5498892664909363, -0.16321992874145508, 0.19026926159858704, -0.724215030670166, 0.30126824975013733, -0.34587690234184265, 0.7089745998382568, 0.19202065467834473, -0.07805976271629333, 0.15745744109153748, 0.20989900827407837, -0.5086807012557983, -0.46209391951560974, 0.7770373225212097, 0.0018942684400826693, -0.010931730270385742, 0.04856023192405701, 0.14760155975818634, -0.6907402873039246, 0.31821855902671814, -0.1687096655368805, -0.2443719357252121, -0.3658149242401123, -0.14698070287704468, -0.37902358174324036, -0.3585938811302185, -0.15092810988426208, 0.6601823568344116, 0.42555034160614014, -0.32914966344833374, -0.31899866461753845, -0.013971354812383652, -0.22896333038806915, -0.3634462356567383, 0.1511450707912445, -0.20370280742645264, -0.3068326711654663, -0.18142245709896088, -0.29759034514427185, 0.1550377905368805, -0.37390753626823425, 0.45349234342575073, 0.6686559915542603, -0.04473710060119629, 0.04563325643539429, 0.18696251511573792, -0.6952769160270691, -0.42387720942497253, 0.48504745960235596, 0.636060357093811, -0.3624493181705475, 0.15627488493919373, 0.6031672954559326, -0.6278428435325623, -0.9024701118469238, 0.4633399546146393, 0.6139535903930664, 0.1988850086927414, 0.2201712727546692, 0.08421045541763306, 0.5652143955230713, 0.5338183045387268]\n",
      "128\n"
     ]
    }
   ],
   "source": [
    "networks_weights = []\n",
    "for keys in mf_dict.keys():\n",
    "    if \"bias\" not in keys:\n",
    "        print(keys)\n",
    "        print(mf_dict[keys].flatten())\n",
    "        print(mf_dict[keys].flatten().shape)\n",
    "        networks_weights.append(mf_dict[keys].flatten().tolist())\n",
    "print()\n",
    "#print(networks_weights)\n",
    "networks_weights = [item for sublist in networks_weights for item in sublist]\n",
    "print(networks_weights)\n",
    "print(len(networks_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use AutoPytorch to search for MLP architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.ndarray size changed, may indicate binary incompatibility. Expected 88 from C header, got 80 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9ce071e84c2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#%pip list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#from autoPyTorch.api.tabular_classification import TabularClassificationTask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mautoPyTorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoNetRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mautoPyTorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_management\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_manager\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataManager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/autoPyTorch-0.0.2-py3.7.egg/autoPyTorch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhpbandster\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mautoPyTorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautonet_classes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoNetClassification\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoNetMultilabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoNetRegression\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoNetImageClassification\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoNetImageClassificationMultipleDatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mautoPyTorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_management\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_manager\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataManager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mautoPyTorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameter_search_space_update\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHyperparameterSearchSpaceUpdates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/autoPyTorch-0.0.2-py3.7.egg/autoPyTorch/core/autonet_classes/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mautoPyTorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautonet_classes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautonet_feature_classification\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoNetClassification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mautoPyTorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautonet_classes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautonet_feature_regression\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoNetRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mautoPyTorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautonet_classes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautonet_feature_multilabel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoNetMultilabel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mautoPyTorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautonet_classes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautonet_image_classification\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoNetImageClassification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mautoPyTorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautonet_classes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautonet_image_classification_multiple_datasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoNetImageClassificationMultipleDatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/autoPyTorch-0.0.2-py3.7.egg/autoPyTorch/core/autonet_classes/autonet_feature_classification.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mautoPyTorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautonet_classes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautonet_feature_data\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoNetFeatureData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mAutoNetClassification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAutoNetFeatureData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mpreset_folder_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"feature_classification\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/autoPyTorch-0.0.2-py3.7.egg/autoPyTorch/core/autonet_classes/autonet_feature_data.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0m__license__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"BSD\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mautoPyTorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoNet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mAutoNetFeatureData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAutoNet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/autoPyTorch-0.0.2-py3.7.egg/autoPyTorch/core/api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mautoPyTorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mautoPyTorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot_encoding\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOneHotEncoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/autoPyTorch-0.0.2-py3.7.egg/autoPyTorch/pipeline/base/pipeline.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mautoPyTorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline_node\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPipelineNode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mautoPyTorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConfigSpace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mautoPyTorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfigspace_wrapper\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConfigWrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/autoPyTorch-0.0.2-py3.7.egg/autoPyTorch/pipeline/base/pipeline_node.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mConfigSpace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mautoPyTorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_option\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConfigOption\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/ConfigSpace/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m ]\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mConfigSpace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfiguration_space\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConfiguration\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mConfigurationSpace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mConfigSpace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameters\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCategoricalHyperparameter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mConfigSpace/configuration_space.pyx\u001b[0m in \u001b[0;36minit ConfigSpace.configuration_space\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mConfigSpace/hyperparameters.pyx\u001b[0m in \u001b[0;36minit ConfigSpace.hyperparameters\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: numpy.ndarray size changed, may indicate binary incompatibility. Expected 88 from C header, got 80 from PyObject"
     ]
    }
   ],
   "source": [
    "#%pip list\n",
    "#from autoPyTorch.api.tabular_classification import TabularClassificationTask\n",
    "from autoPyTorch import AutoNetRegression\n",
    "from autoPyTorch.data_management.data_manager import DataManager\n",
    "\n",
    "\n",
    "# Note: You can write your own datamanager! Call fit train, valid data (numpy matrices) \n",
    "#dm = DataManager()\n",
    "#dm.generate_regression(num_features=21, num_samples=1500)\n",
    "\n",
    "# Note: every parameter has a default value, you do not have to specify anything. The given parameter allow a fast test.\n",
    "#autonet = AutoNetRegression(budget_type='epochs', min_budget=1, max_budget=9, num_iterations=1, log_level='info')\n",
    "\n",
    "#res = autonet.fit(X_train=X_train, Y_train=y_train, X_valid=X_val, Y_valid=y_val)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(equations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'-1.8*x*x+2*x+3.2'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "equation_data.equation_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project-venv",
   "language": "python",
   "name": "project-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
